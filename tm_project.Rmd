---
title: "Popular Songs' Lyrics Analysis"
author: "Phong Duong"
date: "2024-02-14"
output: html_document
---

# Introduction

Since the incident of COVID-19, the daily life of many people has changed, along with this is an increase in mental health issue and a reduction in ability to cope with emotional challenges (search for evidence later). One device that is arguably important to emotional well being is music. Thus, people's choice of music is telling of how they feel about their emotional life as well as their perception of the current world.

In this analysis, we will analyse lyrics from the top popular songs from 2019 to 2023 to hopefully get a glimpse of the emotional experiences and the important topics that are the focal point of a large community of music listeners. The songs are curated from Billboard annual top 100 singles and therefore might not be inclusive to all listeners but nevertheless is representative to a large chunk of the music community (stats?).

# Library

```{r}
#installing packages if needed
install.packages("tidyverse")
install.packages("rvest") #for scraping data from static web pages
install.packages("xml2") #for reading html files of the web pages
remotes::install_github("giovanni-cutri/geniusr") #api of genius website - contains lyrics and many information about songs and artists
install.packages("cld3") #for detecting language
install.packages("tidytext")
install.packages("hunspell") #detecting misspelling
install.packages("ggpubr") #combining ggplots into 1
```

```{r}
library(tidyverse) 
library(rvest) 
library(xml2) 
library(geniusr) 
library(tidytext)
library(hunspell)
library(ggpubr)
```

Note: the package `geniusr` is an R package for using Genius API. The package is originally created by ewenme but there is an issue for function `get_lyrics`, which has been fixed by giovanni-cutri so we will use their version of `geniusr` package.

# Data Collection

The Billboard Year End Hot 100 singles compiles the top performing singles of the United States based on both physical and digital sales. Although the charts are posted on the website of Billboard, in this analysis, we are getting the data from Wikipedia since it has a much better format for scraping the data and it is open-source. We begin the data collection process by making a test scrape for the year 2023.

## Test Scrape 2023

```{r}
#url of wikipedia page for top songs 2023
billboard100_2023 <- "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2023"

#check and clean scraped data and put into a readable format (dataframe)
read_html(billboard100_2023) %>% 
  xml_find_all("//table[contains(@class,'wikitable')]/tbody/tr[td]") %>% 
  xml_text() %>% 
  str_replace_all("\n", "") %>% 
  str_replace_all("\"", "%") %>% 
  as_data_frame() %>% 
  separate_wider_delim("value","%", names = c("position", "song_name", "artists"))
```

The table containing the information that we need is a unique `<table>` element with the `class = 'wikitable'` and all the pages for other years have the same format so we can use the same syntax to scrape songs from all the years that we are interested in.

## Scrape All Songs

```{r}
#interested years
years <- c(2019:2023) 

#helper function to streamline the scraping process
get_top_songs <- function(year){
  url <- paste0("https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_", year)
  
  #save the scraped and cleaned data into a dataframe like previously
  df <- read_html(url) %>% 
  xml_find_all("//table[contains(@class,'wikitable')]/tbody/tr[td]") %>% 
  xml_text() %>% 
  str_replace_all("\n", "") %>% 
  str_replace_all("\"", "%") %>% 
  as_data_frame() %>% 
  separate_wider_delim("value","%", names = c("position", "song_name", "artists"))
  
  df <- df %>% mutate("year" = year) #add a year column
  
  return(df)
}

#applying the helper function to a vector and returning a dataframe
all_top_songs <- map_dfr(years, get_top_songs)

all_top_songs
```

The final data frame has 500 rows, which is in accordant to 5 charts of the 5 years period (2019-2023) so the scraping should have gone correctly. The next step will be to get the lyrics for these songs in order to create a complete data frame. We will use Genius API to get the songs' information and lyrics. We will use the library `geniusr` which facilitates the use of Genius API.

**Important**: To reproduce this report using `geniusr` package, it is important to follow these steps from the guide of the package:

1.  [Create a Genius API client](https://genius.com/api-clients/new)
2.  Generate a client access token from the [API Clients Page](https://genius.com/api-clients)
3.  Calling `genius_token()` and enter your Genius Client Access Token

The full guide to the `geniusr` package is on this [GitHub page](https://github.com/ewenme/geniusr?tab=readme-ov-file)

## Get Lyrics

```{r}
genius_token() #call and enter Client Access Token to authenticate to use Genius API 
```

```{r}
#helper function to get lyrics of a song
just_lyrics <- function(artist, song){
  #search for id of song based on song name and name of first artist
  id <- search_song(paste(song, str_extract(artist, "\\w+( [A-Z]\\w+)?")))[1,1] %>% pull()
  
  #get lyrics based on id
  get_lyrics_id(id) %>% 
    .$line %>% 
    paste(., collapse= " ")
}

#add lyrics column to the data frame of songs from before
all_songs_lyrics <- all_top_songs %>% 
  mutate(
    lyrics = mapply(just_lyrics, artists, song_name)
  )

#save the complete data frame into a csv file for easy retrival for analysis later
write_csv(all_songs_lyrics, "all_top_songs_lyrics.csv")
```

One way to get lyrics of a song using the `geniusr` package is to use the function `get_lyrics_id()` with a singular parameter `id` . This `id` can be retrieved by using the function `search_song` and the resulting table's first cell ([1,1]) is the id of the song. We built a helper function to facilitate the lyrics retrieval.

It took 31 minutes to scrape all the lyrics for the 500 songs so we will save the data into a csv file to avoid scraping every time we perform analysis. *This might make the reproduction of the analysis faulty to changes added after the publish date of the project.*

# Data Cleaning

```{r}
all_songs_lyrics <- read.csv("all_top_songs_lyrics.csv") %>% 
  drop_na()

wrong_lyrics <- c("Ritmo (Bad Boys for Life)", "My Ex's Best Friend", "Hrs and Hrs")

all_songs_lyrics <- all_songs_lyrics %>% 
  filter(!song_name %in% wrong_lyrics) 
```

The lyrics scraped are not perfect because some searches returned id of a remix of the song or a different language version of the song. Therefore, we have to find and clean them. After going through the lyrics of 500 songs, I found 3 songs with faulty lyrics. I decided to exclude them from the list since at this point I had already reached the limit from Genius API.

```{r}
songs_en <- all_songs_lyrics %>% 
  mutate(
    lang = cld3::detect_language(lyrics)
  ) %>% 
  filter(lang == "en")
```

There are 25 songs in other languages than English so we either have to exclude them or translate them into English for the analysis method that we will use in this project. The detection was done by the `cld3` package - a neural network model for language identification. It was mentioned that the version used at the time of this project was still experimental so reproduction might produce different results. Another option to do this task could be using DeepL API. The package `deeplr` is available to facilitate the use of DeepL API and has functions for detecting and translating languages. However, DeepL free API is extremely limited and subscription is expensive. Therefore, for the scope of this project, we will use `cld3` package and exclude songs that are not in English.

# Split and Tokenize

```{r}
songs_en_st <- songs_en %>% 
  unnest_tokens(word, lyrics)
```

As per usual with most case of analysing songs of pop culture, there are many potential issues for the analysis with the word used in these songs. In our case, there are many slang words that are not recorded in the dictionary and words that are sang differently for the purpose of rhyming or simply style. Some examples of these words are "nothing" becoming "nothin", "fighting" becoming "fightin", and so on. We will try to clean these as much as possible but it is frankly very messy to handle. There are packages that can help correcting the misspelling (e.g., `textclean`, `hunspell`) but these are usually not accurate for slang and words from pop culture. Stemming and lemmatization can help but at the same time might take away valuable information or incorrectly stem or lemmatize (for example, some packages reduce "is" to "i" or give errors to slang and words related to pop or internet cluture).

```{r}
misspelling_dict <- songs_en_st %>% 
  select(word) %>% 
  distinct() %>% 
  mutate(misspelling = hunspell_check(word)) %>% 
  filter(misspelling == FALSE) %>% 
  #important misspelling are verb-ing words so we will correct by adding g at the end of word
  mutate(ing_word = str_detect(word, pattern = "\\w+in$")) %>% 
  filter(ing_word == TRUE) %>% 
  mutate(correction = paste0(word, "g")) %>% 
  #double check again to include only words correctly corrected
  mutate(misspelling_2 = hunspell_check(correction)) %>% 
  filter(misspelling_2) %>% 
  select(word, correction)
misspelling_dict
```

We will create a dictionary for misspelled words from the songs' lyrics of the data set. We will use the `hunspell` package for identifying the misspelling. Most of the identified misspelled words are either verbs ending in "ing" (present participle or gerund) and names or words from other languages or slang. Since names and pop culture words contribute meaning to the song and are not entirely affected by the analysis, we will only correct the verb-ing words so later they can be identified by the lexicons. This was done by detecting misspelled words ending with "in" then adding "g" to the of the word. They are checked again by the `hunspell_check` to filter out other words that also end with "in" but are not verb-ings.

```{r}
songs_cleaned <- songs_en_st %>% 
  left_join(misspelling_dict, by = join_by(word)) %>% 
  mutate(correct_word = ifelse(is.na(correction), word, correction)) %>% 
  select(-word, -correction) %>% 
  rename(word = correct_word)

songs_cleaned
```

We prepare the final, cleaned version of the data set by joining the misspelling dictionary data frame with the original data frame then create a new column that takes the correction form of misspelled words and original form if they are not in the dictionary.

# Filter Stop Words

```{r}
songs_en_filtered <- songs_cleaned %>% 
  anti_join(stop_words)

songs_en_filtered
```

# Lexicon-based Sentiment Analysis 

In the following section, we will explore the sentimental themes of the popular songs from 2019 to 2023 to uncover insights about music listeners, as least for the pop culture, and how they relate or reflect the phenomena happening during these years. Since the uneventful arrival of COVID-19, there was a range of emotions displayed on social media, news, and public opinion sharing platforms. The pandemic claimed many lives and forced most countries into isolating people, thus, we could expect a rise in more negative songs about loneliness or sadness. It is also possible that songs about courage and felicity will rise instead as people try to cope by listening to positive songs.

```{r}
sa_df_bing <- songs_en_filtered %>% 
  inner_join(get_sentiments("bing")) %>% 
  group_by(year, song_name, artists) %>% 
  count(sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  mutate(sentiment_label = ifelse(sentiment == 0, "neutral", ifelse(sentiment > 0, "positive", "negative")))

sa_df_bing$sentiment %>% summary()
```

```{r}
sa_df_afinn <- songs_en_filtered %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(year, song_name, artists) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(sentiment_label = ifelse(sentiment == 0, "neutral", ifelse(sentiment > 0, "positive", "negative")))

sa_df_afinn$sentiment %>% summary()
```

We are using 2 lexicons "bing" and "afinn" to measure the sentiments of words of the songs. With "bing", a song is classified as "positive" if the net value of positive words subtracting negative words is greater than 0 (positive - negative \> 0), "negative" if it is below 0 (positive - negative \< 0), and "neutral" otherwise (positive - negative = 0). With "afinn", this sentiment label is categorized by the sum sentimental values of words with the same criteria for "bing".

A brief descriptive analysis on the sentiment attribute shows that more than half of the songs carry negative connotation in their lyrics with means of -8.3 and -16.8 and medians of -6 and -5 for "bing" and "afinn" repsectively. It also shows that the negative notation has a peaked value almost double the positive (-83 compared to 42 for "bing" and -325 compared to 163 for "afinn"). This means that the song with the highest negative connotation has almost double the amount of negative words or negativity than the top positive song. We can see this more clearly with the box-plots below.

```{r}
#box-plot for sentiment column
ggarrange(
  ggplot(sa_df_bing) + geom_boxplot(aes(y = sentiment)),
  ggplot(sa_df_afinn) + geom_boxplot(aes(y = sentiment)),
  labels = c("bing", "afinn")
)
```

## Song sentiment over the years

```{r}
#combine afinn and bing data set to compare song sentiment over the years
oty <- bind_rows(
  sa_df_afinn %>% mutate(method = "afinn"),
  sa_df_bing %>% select(-negative, -positive) %>% mutate(method = "bing")
)

oty %>% 
  group_by(method, year, sentiment_label) %>% 
  count() %>% 
  ggplot() +
  ggalt::geom_xspline(aes(year, n, color = sentiment_label)) + #geom_xspline for smooth line
  xlab("year") +
  ylab("number of songs") +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

## Most common positive and negative words

```{r}
common_words_bing <- songs_en_filtered %>%
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE)

ggplot(common_words_bing %>% filter(n > 50)) +
  geom_col(aes(reorder(word,n), n, fill = sentiment)) +
  coord_flip() +
  facet_wrap(~sentiment, scales = "free_y") +
  xlab("frequency") +
  ylab("word")
```

```{r}
common_words_afinn <- songs_en_filtered %>%
  inner_join(get_sentiments("afinn")) %>% 
  count(word, value, sort = TRUE) %>% 
  mutate(sentiment = case_when(
    value < 0 ~ "negative",
    value > 0 ~ "positive",
    TRUE ~ "neutral"
  ))

ggplot(common_words_afinn %>% filter(n > 50)) +
  geom_col(aes(reorder(word,n), n, fill = sentiment)) +
  coord_flip() +
  facet_wrap(~sentiment, scales = "free_y") +
  xlab("frequency") +
  ylab("word")
```

## Most positive vs most negative song

```{r}
most_pos <- sa_df_afinn %>% 
  group_by(year) %>% 
  slice_max(sentiment)

most_pos
```

```{r}
most_neg <- sa_df_afinn %>% 
  group_by(year) %>% 
  slice_min(sentiment)

most_neg
```

```{r}
ggplot() +
  geom_col(data = most_pos, aes(year, sentiment), fill = "orange") +
  geom_col(data = most_neg, aes(year, sentiment), fill = "blue") +
  coord_flip() +
  geom_text(data = most_pos, aes(x = year, y = 0, label = song_name), 
            hjust = "left", nudge_y = 5) +
  geom_text(data = most_neg, aes(x = year, y = 0, label = song_name), 
            hjust = "right", nudge_y = -5)
```

## Top Artists

```{r}
sa_df_afinn %>% 
  group_by(artists) %>% 
  count(label = sentiment_label, sort = TRUE) %>% 
  head(10)
```

```{r}
sa_df_bing %>% 
  group_by(artists) %>% 
  count(label = sentiment_label, sort = TRUE) %>% 
  head(10)
```

```{r}
artists_sa <- bind_rows(
  sa_df_afinn %>% group_by(artists) %>% count(label = sentiment_label) %>% mutate(method = "afinn"),
  sa_df_bing %>% group_by(artists) %>% count(label = sentiment_label) %>% mutate(method = "bing")
) %>%
  group_by(method, artists) %>% 
  mutate(total_songs = sum(n)) %>% 
  group_by(method) %>% 
  slice_max(n=20, order_by = total_songs, with_ties = TRUE) 
  
artists_sa %>% ggplot() +
  geom_col(aes(reorder(artists, total_songs), n, fill = label)) +
  coord_flip() +
  facet_wrap(~method, scales = "free_y") +
  xlab("Top artists") +
  ylab("number of songs")
```

# Term Frequency Analysis

## Most often used words

```{r}
song_words <- songs_cleaned %>% 
  count(position, year, song_name, word, sort = TRUE) %>% 
  group_by(year, song_name) %>% 
  mutate(total_words = sum(n),
         term_freq = n/total_words,
         position_level = ifelse(position <= 10, "Top 10", "Top 100")) 
song_words
```

```{r}
#word count distribution
ggplot(song_words) +
  geom_histogram(aes(total_words, fill = position_level)) +
  xlab("Total word count per song") +
  ylab("Frequency")
```

## Wordiest songs

```{r}
song_words %>% 
  group_by(year) %>% 
  slice_max(total_words) %>% 
  distinct(song_name, total_words)
```

```{r}
ggplot(song_words) +
  stat_summary(aes(year, total_words),
               fun.y = "mean",
               geom = "bar") +
  facet_wrap(~position_level)
```

## Term Variation

```{r}
ggplot(song_words, aes(term_freq)) +
  geom_histogram() +
  xlim(0, 0.15)
```

```{r}
ggplot(song_words, aes(term_freq)) +
  geom_histogram(show.legend = FALSE) +
  xlim(0, 0.05) + 
  facet_wrap(~year, scales = "free_y")
```

It's possible after COVID-19 and the quarantine period, new words came out so there are less variation in 2022 and 2023.

## Songs with most distinctive and characteristic words

```{r}
songs_tf_idf <- song_words %>% 
  bind_tf_idf(word, song_name, n) %>% 
  group_by(song_name) %>% 
  mutate(total_tf_idf = sum(tf_idf)) %>% 
  arrange(desc(total_tf_idf)) %>% 
  ungroup()

songs_tf_idf 
```

## Distinctive and characteristic words by year

```{r}
songs_tf_idf %>% 
  group_by(year) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>% 
  ggplot(aes(tf_idf, reorder(word, tf_idf))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~year, scale = "free") +
  labs(x = "tf-idf", y = NULL)
```

Maybe important "words" are catchy, repetitive sounds (e.g., da, taki, doo, blrrrd etc.)/

## Comparing Top 10 songs vs the rest

```{r}
songs_tf_idf %>% 
  group_by(position_level) %>% 
  slice_max(tf_idf, n = 10) %>% 
  ungroup() %>% 
  ggplot(aes(tf_idf, reorder(word, tf_idf))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~position_level, scale = "free") +
  labs(x = "tf-idf", y = NULL)
```
